---
title: "Log Trees"
author: "Kevin Foley"
date: "`r Sys.Date()`"
output: word_document
---

```{r 2. Library, include=FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(mice)
library(VIM)
library(ranger)
library(RColorBrewer)
library(rpart)
library(rattle)
library(e1071)
library(xgboost)
library(usemodels)
library(nnet) #our neural network package
library(stacks)
```

```{r 3. read file and clean the data}

train = read.csv("C:/Users/ducat/Documents/UNCW_Grad_school/MIS 502 Predictive Analytics/Final Project2/Stacked Final individual pieces/ames_student-1.csv")

train = train %>% mutate_if(is.character,as_factor) #%>% 

```


Now we'll split the data.   
```{r 5. solit seed and split}
set.seed(123) 
df_split = initial_split(train, prop = 0.7, strata = Above_Median) #70% in training
train = training(df_split)
test = testing(df_split)

# train1 = train
# test1 = test
```

Set-up our folds
```{r  6. set up folds}
set.seed(123)
folds = vfold_cv(train, v = 5)

#summary(train)
```

Let's build all the models: A classification tree, a random forest, and an XGB model. First, and tune them.    
```{r 7. build recipes}

failure_recipe = recipe(Above_Median ~ ., train) %>%  
                step_normalize(all_predictors(),- all_nominal()) %>% # normalizes numeric predictors, not needed for categorical
                step_dummy(all_nominal(), -all_outcomes()) # to anything that is nominal(), but not the response
                           # all_numeric_predictors()) #<- another possible
                        # step_impute_mean(all_numeric_predictors()) %>%

failure_recipe1 = recipe(Above_Median ~ Neighborhood +Alley +  Bldg_Type + House_Style +Overall_Cond + Foundation + Bsmt_Exposure + Full_Bath + Half_Bath + TotRms_AbvGrd + Fireplaces + Garage_Type + Garage_Cars +  Screen_Porch + Total_Bsmt_SF + Bsmt_Full_Bath+Sale_Type+Sale_Condition+Misc_Feature, train) %>%
   step_normalize(all_predictors(),- all_nominal()) %>% # normalizes numeric predictors, not needed for categorical
     step_other(Neighborhood, threshold = 0.02) %>% #collapses small Neighborhoods into an "Other" group
     step_other(c(House_Style,Foundation,Overall_Cond,Sale_Condition,Sale_Type),threshold = 0.05 ) %>% 
    #step_other(Foundation, threshold = 0.01) %>% 
     step_dummy(all_nominal(), -all_outcomes()) 
                       
ctrl_grid = control_stack_grid() #necessary for working with the stacks package
ctrl_res = control_stack_resamples() #necessary for working with the stacks package
```

```{r   str and summary, include =FALSE}
str(train)
summary(train)
```

# Tree Model 1

```{r 9. Tree Model 1 , eval=FALSE }
 start_tm_time <- Sys.time()
 tree_model = decision_tree(cost_complexity = tune()) %>%
   set_engine("rpart", model = TRUE) %>%
   set_mode("classification")

 tree_recipe = failure_recipe # %>%
#   step_normalize(all_predictors(), -all_nominal()) #+
#   #step_dummy(all_nominal(),-all_outcomes())
#
 tree_workflow = workflow() %>%
   add_model(tree_model) %>%
   add_recipe(tree_recipe)

 set.seed(1234)
 tree_res =
   tree_workflow %>%
   tune_grid(
     resamples = folds,
     grid = 25, #try 25 reasonable values for cp
     control = ctrl_grid #needed for stacking
     )
 end_tm_time <- Sys.time()
 end_tm_time - start_tm_time

 saveRDS(tree_res,"tree_res.rds")  # takes about(1 min)
```
```{r}
tree_res = readRDS("tree_res.rds")
```
```{r 10. Parameter tuning (iterative tuning)}
tree_res %>%
  collect_metrics() %>%
  ggplot(aes(cost_complexity, mean)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  labs(title="Descision Tree", subtitle="failure_recipe")
```


# Tree Model2

```{r 9. Tree Model2, eval=FALSE }
start_tm_time <- Sys.time()
tree_model = decision_tree(cost_complexity = tune()) %>%
   set_engine("rpart", model = TRUE) %>%
   set_mode("classification")

tree_recipe1 = failure_recipe1 # %>%
#   step_normalize(all_predictors(), -all_nominal()) #+
#   #step_dummy(all_nominal(),-all_outcomes())
#
tree_workflow = workflow() %>%
   add_model(tree_model) %>%
   add_recipe(tree_recipe1)

set.seed(1234)
tree_res1 =
   tree_workflow %>%
   tune_grid(
     resamples = folds,
     grid = 25, #try 25 reasonable values for cp
     control = ctrl_grid #needed for stacking
     )
end_tm_time <- Sys.time()
end_tm_time - start_tm_time

saveRDS(tree_res1,"tree_res1.rds")  # takes about(1 min)
```
```{r}
tree_res1 = readRDS("tree_res1.rds")
```
```{r 10. Parameter tuning (iterative tuning)}
tree_res1 %>%
  collect_metrics() %>%
  ggplot(aes(cost_complexity, mean)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2)+
  labs(title="Descision Tree", subtitle="failure_recipe1")
```