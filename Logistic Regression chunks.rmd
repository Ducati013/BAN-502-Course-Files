---
title: "Logistic Regression"
author: "Kevin Foley"
date: "`r Sys.Date()`"
output: word_document
---

```{r 2. Library, include=FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(mice)
library(VIM)
library(ranger)
library(RColorBrewer)
library(rpart)
library(rattle)
library(e1071)
library(xgboost)
library(usemodels)
library(nnet) #our neural network package
library(stacks)
```

```{r 3. read file and clean the data}

train = read.csv("C:/Users/ducat/Documents/UNCW_Grad_school/MIS 502 Predictive Analytics/Final Project2/Stacked Final individual pieces/ames_student-1.csv")

train = train %>% mutate_if(is.character,as_factor) #%>% 
#                   mutate(failure = as_factor(failure)) %>% 
#                   mutate(failure = fct_recode(failure, "No" = "0", "Yes" = "1" )) %>% 
#                   mutate(attribute_2 = as_factor(attribute_2)) %>% 
#                   mutate(attribute_3 = as_factor(attribute_3)) 


# test = test %>% mutate_if(is.character,as_factor) %>% 
#                   mutate_if(is.integer,as_factor) #%>% 
#                   #mutate(failure = fct_recode(failure, "No" = "0", "Yes" = "1" )) 
#                   
# test$id = as.integer(test$id)  #convert is col back to int
#  
 
```

Now we'll split the data.   
```{r 5. solit seed and split}
set.seed(123) 
df_split = initial_split(train, prop = 0.7, strata = Above_Median) #70% in training
train = training(df_split)
test = testing(df_split)

# train1 = train
# test1 = test
```

Set-up our folds
```{r  6. set up folds}
set.seed(123)
folds = vfold_cv(train, v = 5)

#summary(train)
```

Let's build all the models: A classification tree, a random forest, and an XGB model. First, and tune them.    
```{r 7. build recipes}

df_recipe = recipe(Above_Median ~ ., train) %>%  
                step_normalize(all_predictors(),- all_nominal()) %>% # normalizes numeric predictors, not needed for categorical
                step_dummy(all_nominal(), -all_outcomes()) # to anything that is nominal(), but not the response
                           # all_numeric_predictors()) #<- another possible
                        # step_impute_mean(all_numeric_predictors()) %>%

failure_recipe1 = recipe(Above_Median ~ Neighborhood +Alley +  Bldg_Type + House_Style +Overall_Cond + Foundation + Bsmt_Exposure + Full_Bath + Half_Bath + TotRms_AbvGrd + Fireplaces + Garage_Type + Garage_Cars +  Screen_Porch + Total_Bsmt_SF + Bsmt_Full_Bath+Sale_Type+Sale_Condition+Misc_Feature, train) %>%
   step_normalize(all_predictors(),- all_nominal()) %>% # normalizes numeric predictors, not needed for categorical
     step_other(Neighborhood, threshold = 0.02) %>% #collapses small Neighborhoods into an "Other" group
     step_other(c(House_Style,Foundation,Overall_Cond,Sale_Condition,Sale_Type),threshold = 0.05 ) %>% 
    #step_other(Foundation, threshold = 0.01) %>% 
     step_dummy(all_nominal(), -all_outcomes()) 
                       
ctrl_grid = control_stack_grid() #necessary for working with the stacks package
ctrl_res = control_stack_resamples() #necessary for working with the stacks package
```

```{r   str and summary, include =FALSE}
str(train)
summary(train)
```

# Log Reg 1
```{r 8. build the logistic regression model and summary, eval=TRUE, include =FALSE}
start_lr_time = Sys.time()
failure_model =
   logistic_reg() %>% #note the use of logistic_reg
   set_engine("glm") #standard logistic regression engine is glm

# # failure_recipe = recipe(failure ~ ., train) %>%   # recipe(response(y)~ predictors(x1+x2), df)
# #   step_dummy(all_nominal(), -all_outcomes()) #exclude the response variable from being dummy converted
#
logReg_wf = workflow() %>%
   add_recipe(df_recipe) %>%
   add_model(failure_model)

logReg_fit = fit(logReg_wf, train)

#
set.seed(1234)
# LogReg_res = 
#    logReg_wf %>% 
#    tune_grid(
#      resamples = folds,
#      grid = 25, #try 25 reasonable values for cp
#      control = ctrl_grid #needed for stacking
#      )

end_lr_time <- Sys.time()
end_lr_time - start_lr_time
#
saveRDS(logReg_fit,"LogReg_fit.rds")

summary(logReg_fit$fit$fit$fit)


```

```{r}
logReg_fit = readRDS("LogReg_fit.rds")
```


# Log reg 2
```{r}
start_lr_time = Sys.time()
# recipe
# failure_recipe1 = recipe(Above_Median ~ Neighborhood +Alley +  Bldg_Type + House_Style +Overall_Cond + Foundation + Bsmt_Exposure + Full_Bath + Half_Bath + TotRms_AbvGrd + Fireplaces + Garage_Type + Garage_Cars +  Screen_Porch + Total_Bsmt_SF + Bsmt_Full_Bath+Sale_Type+Sale_Condition+Misc_Feature, train) %>%  
#    step_normalize(all_predictors(),- all_nominal()) %>% # normalizes numeric predictors, not needed for categorical
#    step_dummy(all_nominal(), -all_outcomes())

failure_model =
   logistic_reg() %>% #note the use of logistic_reg
   set_engine("glm") #standard logistic regression engine is glm

# # failure_recipe = recipe(failure ~ ., train) %>%   # recipe(response(y)~ predictors(x1+x2), df)
# #   step_dummy(all_nominal(), -all_outcomes()) #exclude the response variable from being dummy converted
#
logReg_wf = workflow() %>%
   add_recipe(failure_recipe1) %>%
   add_model(failure_model)

logReg_fit1 = fit(logReg_wf, train)

#
# set.seed(1234)
# LogReg_res1 = 
#    logReg_wf %>% 
#    tune_grid(
#      resamples = folds,
#      grid = 25, #try 25 reasonable values for cp
#      control = ctrl_grid #needed for stacking
#      )

end_lr_time <- Sys.time()
end_lr_time - start_lr_time
#
saveRDS(logReg_fit1,"logReg_fit1.rds")

summary(logReg_fit1$fit$fit$fit)



```
```{r statistics visuals, include =TRUE}

train %>% group_by(Neighborhood) %>% summarize(freq = n()) %>% arrange(desc(freq))

```

