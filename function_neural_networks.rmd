---
title: "Neural Networks functions"
author: "Kevin Foley"
date: "`r Sys.Date()`"
output: word_document
---
# functions
```{r  all the intro stuff, echo=FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(mice)
library(VIM)
library(ranger)
library(RColorBrewer)
library(rpart)
library(rattle)
library(e1071)
library(xgboost)
library(usemodels)
library(nnet) #our neural network package
library(stacks)
library(vip) #variable importance

train = read.csv("C:/Users/ducat/Documents/UNCW_Grad_school/MIS 502 Predictive Analytics/Final Project2/Stacked Final individual pieces/ames_student-1.csv")

train = train %>% mutate_if(is.character,as_factor) 
set.seed(123) 
df_split = initial_split(train, prop = 0.7, strata = Above_Median) #70% in training
train = training(df_split)
test = testing(df_split)
set.seed(123)
folds = vfold_cv(train, v = 5)
recipe1 = recipe(Above_Median ~ ., train) %>%  
                step_normalize(all_predictors(),- all_nominal()) %>% # normalizes numeric predictors, not needed for categorical
                step_dummy(all_nominal(), -all_outcomes()) # to anything that is nominal(), but not the response
                           # all_numeric_predictors()) #<- another possible
                        # step_impute_mean(all_numeric_predictors()) %>%

recipe = recipe(Above_Median ~ Gr_Liv_Area + Year_Built + Full_Bath + First_Flr_SF + Total_Bsmt_SF + Garage_Area + Lot_Area + Garage_Type + Overall_Qual + Garage_Cars, train) %>%
   step_normalize(all_predictors(),- all_nominal()) %>%
     step_dummy(all_nominal(), -all_outcomes()) 
                       
ctrl_grid = control_stack_grid() #necessary for working with the stacks package
ctrl_res = control_stack_resamples() #necessary for working with the stacks package
```


```{r accuracy report, include=FALSE}
# accuracy_report = function(nn_res, workflow, train, test){
# 
# set.seed(123)  # this ensures we all get the same random forest
# best_xgbt_res1 = select_best(nn_res, "accuracy")
# 
# final_xgbt_res1 = finalize_workflow(
#   workflow,
#   best_xgbt_res1
# )
# 
# final_xgbt_res1
# final_xgbt_res_fit1 = fit(final_xgbt_res1, train)
# final_xgbt_res_fit1 %>% pull_workflow_fit() %>% vip(geom = "point")
# trainpredrf = predict(final_xgbt_res_fit1, train)
# auto_train =confusionMatrix(trainpredrf$.pred_class, train$Above_Median, 
#                 positive = "Yes")
# t0 = data.matrix(auto_train)
# auto_train_accuracy = (t0[1,1]+t0[2,2])/nrow(train)   # t0[row,col)]
# auto_train_sensitivity = (t0[2,2]/(t0[2,2]+t0[1,2]))
# testpredrf = predict(final_xgbt_res_fit1, test)
# head(testpredrf)
# auto_test = confusionMatrix(testpredrf$.pred_class, test$Above_Median, 
#                 positive = "Yes")
# t00 = data.matrix(auto_test)
# auto_test_accuracy = (t00[1,1]+t00[2,2])/nrow(test)
# auto_test_sensitivity = (t00[2,2]/(t00[2,2]+t00[1,2]))
# cat(strrep("*",15),"Train & Test: Accuracy & Sensitivity",strrep("*",15),'\n\n')
# cat("Train accuracy:   ", round(auto_train_accuracy,digits=4),
#     "\nTrain sensitivity:", round(auto_train_sensitivity,digits=4),
#     #'\n\n',strrep("*",15),"Test parameters",strrep("*",15),
#      "\n\nTest  accuracy:   ", round(auto_test_accuracy,digits=4),
#     "\nTest sensitivity:", round(auto_test_sensitivity,digits=4),"\n\n")
# # cat(strrep("*",15),"Train parameters",strrep("*",15),'\n')
# # auto_train
# # cat(strrep("*",15),"Test parameters",strrep("*",15),'\n')
# # auto_test
# cat(strrep("*",25),"END",strrep("*",25),'\n')
# }
```

```{r visuals}
# neuralt_visuals = function(neuralt_res){
# 
# ggplot = neuralt_res %>%
#   collect_metrics() %>%
#   filter(.metric == "accuracy") %>%
#   select(mean, hidden_units, penalty, epochs) %>%
#   pivot_longer(hidden_units:epochs,
#     values_to = "value",
#     names_to = "parameter"
#   ) %>%
#   ggplot(aes(value, mean, color = parameter)) +
#   geom_point(show.legend = FALSE) +
#   facet_wrap(~parameter, scales = "free_x") #+
#   # labs(title = "Tuned Neural Network", x = NULL, y = "Accuracy")
# 
# # neuralt_res %>%
# #   collect_metrics() %>%
# #   filter(.metric == "accuracy") %>%
# #   mutate(hidden_units = factor(hidden_units)) %>%
# #   ggplot(aes(penalty, mean, color = epochs)) +
# #   geom_line(alpha = 0.5, linewidth = 1.5) +
# #   geom_point() +
# #   facet_wrap(~hidden_units, ncol =2 ) + 
# #   labs(title = "Tuned Neural Network", y = "Accuracy")
# return(ggplot)
# }
# saveRDS(neuralt_visuals,"fxn_neuralt_visuals.rds")
```

```{r tuned neural network hidden_units(2,6) penalty(-10,-1) epochs(0-600), levels=10)}
# tuned_neural_network = function(recipe,folds){
#   # This model also takes awhile to run. Saved to RDS.
# start_nn_time <- Sys.time()
# #
# neural_grid = grid_regular(
#    hidden_units(range = c(2,6)), # must be whole numbers
#    penalty(range = c(-10,-1)),
#    #penalty is a weird one, you are not setting the actual penalty itself, you are setting the range of x in 10^x
#    epochs(range = c(0,600)),
#    levels = 10
#  )
# #
# nn_recipe = recipe #%>%
#     #step_normalize(all_predictors(), -all_nominal()) #normalize the numeric predictors, not needed for categorical
# #
# nn_model =
#     mlp(hidden_units = tune(), penalty = tune(),
#         epochs = tune()) %>%
#     set_mode("classification") %>%
#     set_engine("nnet", verbose = 0) #verbose = 0 reduces output from the model
# #
# nn_workflow <-
#     workflow() %>%
#     add_recipe(nn_recipe) %>%
#   add_model(nn_model)
# 
# set.seed(1234)
# neuralt_res <-
#     tune_grid(nn_workflow,
#               resamples = folds,
#               grid = neural_grid,
#               control = ctrl_grid)
# #
#  end_nn_time <- Sys.time()
#  end_nn_time - start_nn_time
# 
#  saveRDS(neuralt_res,"neuralt_res.rds")  # about(26 mins)
#  return(nn_workflow)
# }
# saveRDS(tuned_neural_network, "fxn_tuned_neural_network.rds")
```

```{r neural network  grid=25, echo=FALSE}
# neural_network = function(recipe, folds){
# 
#   start_time = Sys.time() #for timing
# 
# # make the model
# nn_model =
#    mlp(hidden_units = tune(), penalty = tune(),
#        epochs = tune()) %>%
#    set_mode("classification") %>%
#    set_engine("nnet", verbose = 0) #verbose = 0 reduces output from the model
# 
# #   make the workflow
# nn_workflow <-
#    workflow() %>%
#    add_recipe(recipe) %>%
#    add_model(nn_model)
# 
# # set the seed and tune the grid
# set.seed(1234)
# neural_res <-
#    tune_grid(nn_workflow,
#              resamples = folds,
#              grid = 25,
#              control = ctrl_grid)
# 
# # save the rds file and return workflow
# end_time = Sys.time()
# end_time-start_time
# 
# saveRDS(neural_res,"neural_res.rds")  # (about 1.56 mins)
# return(nn_workflow)
# }
# saveRDS(neural_network, "fxn_neural_network.rds")
```


# load functions()
```{r load functions,echo=FALSE}
# load functions
neural_network = readRDS("fxn_neural_network.rds")
accuracy_report = readRDS("accuracy_report.rds")
neuralt_visuals= readRDS("fxn_neuralt_visuals.rds")
tuned_neural_network = readRDS("fxn_tuned_neural_network.rds")
```

# MAIN()
```{r Main, echo=FALSE}
# begin neural network model
workflow = neural_network(recipe,folds)
nn_res = readRDS("neural_res.rds")
accuracy_report(nn_res, workflow, train, test)
neuralt_visuals(nn_res)+ labs(title = "Neural Network",subtitle = "recipe1", x = NULL, y = "Accuracy")


# begin tuned neural network model
workflow = tuned_neural_network(recipe, folds)
nnt_res = readRDS("neuralt_res.rds")
accuracy_report(nn_res, workflow, train,test)
neuralt_visuals(nnt_res)+ labs(title = "Tuned Neural Network" ,subtitle = "recipe1", x = NULL, y = "Accuracy")

```

