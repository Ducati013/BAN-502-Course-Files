---
title: "Function_ Logistic Regression"
author: "Kevin Foley"
date: "`r Sys.Date()`"
output: word_document
---
# Function

```{r  all the intro stuff}
library(tidyverse)
library(tidymodels)
library(caret)
library(mice)
library(VIM)
library(ranger)
library(RColorBrewer)
library(rpart)
library(rattle)
library(e1071)
library(xgboost)
library(usemodels)
library(nnet) #our neural network package
library(stacks)
library(vip) #variable importance

train = read.csv("C:/Users/ducat/Documents/UNCW_Grad_school/MIS 502 Predictive Analytics/Final Project2/Stacked Final individual pieces/ames_student-1.csv")

train = train %>% mutate_if(is.character,as_factor) 
set.seed(123) 
df_split = initial_split(train, prop = 0.7, strata = Above_Median) #70% in training
train = training(df_split)
test = testing(df_split)
set.seed(123)
folds = vfold_cv(train, v = 5)
recipe = recipe(Above_Median ~ ., train) %>%  
                step_normalize(all_predictors(),- all_nominal()) %>% # normalizes numeric predictors, not needed for categorical
                step_dummy(all_nominal(), -all_outcomes()) # to anything that is nominal(), but not the response
                           # all_numeric_predictors()) #<- another possible
                        # step_impute_mean(all_numeric_predictors()) %>%

recipe1 = recipe(Above_Median ~ Gr_Liv_Area + Year_Built + Full_Bath + First_Flr_SF + Total_Bsmt_SF + Garage_Area + Lot_Area + Garage_Type + Overall_Qual + Garage_Cars, train) %>%
   step_normalize(all_predictors(),- all_nominal()) %>%
     step_dummy(all_nominal(), -all_outcomes()) 
                       
recipe2 = recipe(Above_Median ~ Gr_Liv_Area + Year_Built + Full_Bath + First_Flr_SF + Total_Bsmt_SF + Garage_Area + Lot_Area + Garage_Type + Overall_Qual + Garage_Cars + Functional +Bsmt_Cond + Bsmt_Qual , train) %>%
   step_normalize(all_predictors(),- all_nominal()) %>%
     step_dummy(all_nominal(), -all_outcomes()) 
recipe3 = recipe(Above_Median ~ Gr_Liv_Area + Year_Built + Full_Bath + First_Flr_SF + Total_Bsmt_SF + Garage_Type + Overall_Qual, train) %>%
   step_normalize(all_predictors(),- all_nominal()) %>%
     step_dummy(all_nominal(), -all_outcomes()) 

ctrl_grid = control_stack_grid() #necessary for working with the stacks package
ctrl_res = control_stack_resamples() #necessary for working with the stacks package
```

```{r log reg function}
fxn_Log_reg = function(recipe, folds){

  start_lr_time = Sys.time()
failure_model =
   logistic_reg() %>% #note the use of logistic_reg
   set_engine("glm") #standard logistic regression engine is glm

# # failure_recipe = recipe(failure ~ ., train) %>%   # recipe(response(y)~ predictors(x1+x2), df)
# #   step_dummy(all_nominal(), -all_outcomes()) #exclude the response variable from being dummy converted
#
logReg_wf = workflow() %>%
   add_recipe(recipe) %>%
   add_model(failure_model)

logReg_fit = fit(logReg_wf, train)

#
set.seed(1234)
# LogReg_res = 
#    logReg_wf %>% 
#    tune_grid(
#      resamples = folds,
#      grid = 25, #try 25 reasonable values for cp
#      control = ctrl_grid #needed for stacking
#      )

end_lr_time <- Sys.time()
end_lr_time - start_lr_time
#
saveRDS(logReg_fit,"LogReg_fit.rds")

summary(logReg_fit$fit$fit$fit)

}
```

# Main()
```{r main}
#fxn_Log_reg(recipe, folds)  # the significance indicators will not show up for the variables when there are too many

fxn_Log_reg(recipe1,folds)

```

