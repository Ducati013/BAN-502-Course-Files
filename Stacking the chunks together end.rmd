---
title: "Stacking Time"
author: "Kevin Foley"
date: "`r Sys.Date()`"
output: word_document
---
# Stacking
```{r 1. main start}
main_start <-Sys.time()
```

```{r 2. Library, include=FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(mice)
library(VIM)
library(ranger)
library(RColorBrewer)
library(rpart)
library(rattle)
library(e1071)
library(xgboost)
library(usemodels)
library(nnet) #our neural network package
library(stacks)
library(vip) #variable importance
```

```{r 3. read file and clean the data}

train = read.csv("C:/Users/ducat/Documents/UNCW_Grad_school/MIS 502 Predictive Analytics/Final Project2/Stacked Final individual pieces/ames_student-1.csv")

train = train %>% mutate_if(is.character,as_factor) 

```

Now we'll split the data.   
```{r 5. solit seed and split}
set.seed(123) 
df_split = initial_split(train, prop = 0.7, strata = Above_Median) #70% in training
train = training(df_split)
test = testing(df_split)

# train1 = train
# test1 = test
```

Set-up our folds
```{r  6. set up folds}
set.seed(123)
folds = vfold_cv(train, v = 5)

#summary(train)
```

Let's build all the models: A classification tree, a random forest, and an XGB model. First, and tune them.    
```{r 7. build recipes}

failure_recipe = recipe(Above_Median ~ ., train) %>%  
                step_normalize(all_predictors(),- all_nominal()) %>% # normalizes numeric predictors, not needed for categorical
                step_dummy(all_nominal(), -all_outcomes()) # to anything that is nominal(), but not the response
                           # all_numeric_predictors()) #<- another possible
                        # step_impute_mean(all_numeric_predictors()) %>%

failure_recipe1 = recipe(Above_Median ~ Gr_Liv_Area + Year_Built + Full_Bath + First_Flr_SF + Total_Bsmt_SF + Garage_Area + Lot_Area + Garage_Type + Overall_Qual + Garage_Cars, train) %>%
   step_normalize(all_predictors(),- all_nominal()) %>%
     step_dummy(all_nominal(), -all_outcomes()) 
                       
ctrl_grid = control_stack_grid() #necessary for working with the stacks package
ctrl_res = control_stack_resamples() #necessary for working with the stacks package
```


# Loading
```{r 8. Loading xgb0}

# Trees
tree_res = readRDS("tree_res.rds")
tree_res1 = readRDS("tree_res1.rds")

# Random Forests
rf_res = readRDS("rf_res.rds")
rf_res1 = readRDS("rf_res1.rds")
rft_res = readRDS("rft_res.rds")
rft_res1 = readRDS("rft_res1.rds")

# Neural Networks
neural_res = readRDS("neural_res.rds")
neural_res1 = readRDS("neural_res1.rds")
neuralt_res = readRDS("neuralt_res.rds")
neuralt_res1 = readRDS("neuralt_res1.rds")

# XGBoost
xgb_res = readRDS("xgb_res.rds")
xgb_res1 = readRDS("xgb_res1.rds")
xgbt_res = readRDS("xgbt_res.rds")
xgbt_res1 = readRDS("xgbt_res1.rds")
```

# Stacking Time
```{r 9.start stacking time}
start_stacking_time <- Sys.time()
failure_stacks = stacks() %>%
  add_candidates(tree_res) %>%  # This is the beginning of the recipe with the significant variables
  add_candidates(rf_res) %>%
  add_candidates(rft_res) %>%
  add_candidates(neural_res) %>%
  add_candidates(neuralt_res) %>%
  add_candidates(xgb_res) %>%
  add_candidates(xgbt_res) %>% 
  add_candidates(tree_res1) %>%  # this is the beginning of the recipe with all the predictors
  add_candidates(rf_res1) %>%
  add_candidates(rft_res1) %>%
  add_candidates(neural_res1) %>%
  add_candidates(neuralt_res1) %>%
  add_candidates(xgb_res1) %>%
  add_candidates(xgbt_res1)
end_stacking_time <- Sys.time()
end_stacking_time - start_stacking_time
```

```{r 10. blendin stacks}
failure_blend = 
  failure_stacks %>% 
  blend_predictions(metric = metric_set(accuracy)) #fits a Lasso model to the stack  
  #setting the metric in the above line is extremely important!!
```
Look at results
```{r 11. look at results }
autoplot(failure_blend, type = "weights")
```
Fit the stack to training data
```{r 12. fit stack to training data}
#Fit the stack on the training set
failure_blend <-
  failure_blend %>%
  fit_members()

saveRDS(failure_blend, "final_blend.rds")
```

#Load RDS
```{r}
failure_blend = readRDS("final_blend.rds")
```


Predictions  
```{r 13. predictions}
trainpredstack = predict(failure_blend, train)
head(trainpredstack)
```

Confusion matrix
```{r 14. confusion matrix}
auto_train=confusionMatrix(trainpredstack$.pred_class, train$Above_Median, 
                positive = "Yes")
t0 = data.matrix(auto_train)
auto_train_accuracy = (t0[1,1]+t0[2,2])/nrow(train)   # t0[row,col)]
auto_train_sensitivity = (t0[2,2]/(t0[2,2]+t0[1,2]))
```

Predictions  
```{r 15. predictions}
testpredstack = predict(failure_blend, test)
head(testpredstack)
```

Confusion matrix
```{r 16. comfusion matrix}
auto_test = confusionMatrix(testpredstack$.pred_class, test$Above_Median, 
                positive = "Yes")
t00 = data.matrix(auto_test)
auto_test_accuracy = (t00[1,1]+t00[2,2])/nrow(test)
auto_test_sensitivity = (t00[2,2]/(t00[2,2]+t00[1,2]))

```

Compare model performance on test set  
```{r 17. } 
test = test %>% bind_cols(predict(failure_blend,.))
```


```{r 18. compare results of stacked model}
#compare the results of the stacked model to the constituent models
member_testpreds =  
  test %>%
  select(Above_Median) %>%
  bind_cols(predict(failure_blend, test, members = TRUE))
```

```{r 19. map them}
map_dfr(member_testpreds, accuracy, truth = Above_Median, data = member_testpreds) %>%
  mutate(member = colnames(member_testpreds))
```




```{r 20. summary output for XGB,  include =TRUE}
#______________________Begin train and test summary_____________________________________
cat(strrep("*",15),"After Stacking Train & Test results for: Accuracy & Sensitivity",strrep("*",15),'\n\n')
cat(" Train accuracy:   ", round(auto_train_accuracy,digits=4),
    "\nTrain sensitivity:", round(auto_train_sensitivity,digits=4),
    '\n\n',strrep("*",15),"Test parameters",strrep("*",15),
      "\n\nTest  accuracy:   ", round(auto_test_accuracy,digits=4),
     "\nTest sensitivity:", round(auto_test_sensitivity,digits=4),"\n\n")
cat('\n\n',strrep("*",15),"No information rate: the random guessing rate: 0.5081",strrep("*",15),'\n')
#auto_train
# cat('\n\n',strrep("*",15),"Test parameters",strrep("*",15),'\n')
as.table(auto_test)  # gives the prediction and reference titles
#as.matrix(auto_test)
```


```{r 21. main timer ends}
main_end <-Sys.time()
main_end-main_start
```

