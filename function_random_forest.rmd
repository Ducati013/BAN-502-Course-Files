# Random forest function

# Functions
```{r 2. Library, include=FALSE, echo=FALSE}
#load_it = function(){
library(tidyverse)
library(tidymodels)
library(caret)
library(mice)
library(VIM)
library(ranger)
library(RColorBrewer)
library(rpart)
library(rattle)
library(e1071)
library(xgboost)
library(usemodels)
library(nnet) #our neural network package
library(stacks)
library(vip) #variable importance

train = read.csv("C:/Users/ducat/Documents/UNCW_Grad_school/MIS 502 Predictive Analytics/Final Project2/Stacked Final individual pieces/ames_student-1.csv")

train = train %>% mutate_if(is.character,as_factor) 
set.seed(123) 
df_split = initial_split(train, prop = 0.7, strata = Above_Median) #70% in training
train = training(df_split)
test = testing(df_split)
set.seed(123)
folds = vfold_cv(train, v = 5)
recipe1 = recipe(Above_Median ~ ., train) %>%  
                step_normalize(all_predictors(),- all_nominal()) %>% # normalizes numeric predictors, not needed for categorical
                step_dummy(all_nominal(), -all_outcomes()) # to anything that is nominal(), but not the response
                           # all_numeric_predictors()) #<- another possible
                        # step_impute_mean(all_numeric_predictors()) %>%

recipe = recipe(Above_Median ~ Gr_Liv_Area + Year_Built + Full_Bath + First_Flr_SF + Total_Bsmt_SF + Garage_Area + Lot_Area + Garage_Type + Overall_Qual + Garage_Cars, train) %>%
   step_normalize(all_predictors(),- all_nominal()) %>%
     step_dummy(all_nominal(), -all_outcomes()) 
                       
ctrl_grid = control_stack_grid() #necessary for working with the stacks package
ctrl_res = control_stack_resamples() #necessary for working with the stacks package
#}
#saveRDS(load_it,"load_it.rds")
```


```{r visuals}
fxn_rf_visuals = function(rf_res){
  rf_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, linewidth = 1.5) +
  geom_point() +
  geom_text(aes(label=ifelse(mean>.925,as.character(min_n),'')),hjust=0,vjust=0)+
  labs(title ="Random Forests 1",subtitle="failure_recipe", y = "Accuracy")

rf_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(labs(title ="Random Forests 1",subtitle="failure_recipe", y = "Accuracy", x = NULL))  

}
saveRDS(fxn_rf_visuals,"fxn_rf_visuals.rds")
```

```{r tuned visuals}
fxn_rf_tuned_visuals = function(rft_res){
  p1 = rft_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  labs(title= "Random Forest Tuned",y = "Accuracy")+
  geom_text(aes(label=ifelse(mean>.925,as.character(min_n),'')),hjust=0,vjust=0)

p2 = rft_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(title ="Random Forests Tuned", y = "Accuracy", x = NULL)
# library(gridExtra) needed for grid.arrange()
p1
p2
}
saveRDS(fxn_rf_tuned_visuals,"fxn_rf_tuned_visuals.rds")
```

```{r accuracy}
accuracy_report = function(rf_res, workflow, train, test){

set.seed(123)  # this ensures we all get the same random forest
best_xgbt_res1 = select_best(rf_res, "accuracy")

final_xgbt_res1 = finalize_workflow(
  workflow,
  best_xgbt_res1
)

final_xgbt_res1
final_xgbt_res_fit1 = fit(final_xgbt_res1, train)
final_xgbt_res_fit1 %>% extract_fit_parsnip() %>% vip(geom = "point")
trainpredrf = predict(final_xgbt_res_fit1, train)
auto_train =confusionMatrix(trainpredrf$.pred_class, train$Above_Median, 
                positive = "Yes")
t0 = data.matrix(auto_train)
auto_train_accuracy = (t0[1,1]+t0[2,2])/nrow(train)   # t0[row,col)]
auto_train_sensitivity = (t0[2,2]/(t0[2,2]+t0[1,2]))
testpredrf = predict(final_xgbt_res_fit1, test)
head(testpredrf)
auto_test = confusionMatrix(testpredrf$.pred_class, test$Above_Median, 
                positive = "Yes")
t00 = data.matrix(auto_test)
auto_test_accuracy = (t00[1,1]+t00[2,2])/nrow(test)
auto_test_sensitivity = (t00[2,2]/(t00[2,2]+t00[1,2]))
cat(strrep("*",15),"Train & Test: Accuracy & Sensitivity",strrep("*",15),'\n\n')
cat("Train accuracy:   ", round(auto_train_accuracy,digits=4),
    "\nTrain sensitivity:", round(auto_train_sensitivity,digits=4),
    #'\n\n',strrep("*",15),"Test parameters",strrep("*",15),
     "\n\nTest  accuracy:   ", round(auto_test_accuracy,digits=4),
    "\nTest sensitivity:", round(auto_test_sensitivity,digits=4),"\n\n")
# cat(strrep("*",15),"Train parameters",strrep("*",15),'\n')
# auto_train
# cat(strrep("*",15),"Test parameters",strrep("*",15),'\n')
# auto_test
cat(strrep("*",25),"END",strrep("*",25),'\n')
}
```

```{r  trees=200, set.seed(1234),grid=200}
fxn_random_forest = function(recipe, folds){  

start_rf_time <-Sys.time()

rf_model = rand_forest(mtry = tune(), min_n = tune(), trees = 200) %>% #add tuning of mtry and min_n parameters
    set_engine("ranger", importance = "permutation") %>% #added importance metric
    set_mode("classification")
#
rf_wflow =
    workflow() %>%
    add_model(rf_model) %>%
    add_recipe(recipe)
#
  set.seed(1234)
  rf_res = tune_grid(
    rf_wflow,
    resamples = folds,
    grid = 200,
    control = ctrl_grid
 )

saveRDS(rf_res,"rf_res.rds") # ( takes about 16.55 mins)
return(rf_wflow)
end_rf_time <- Sys.time()
end_rf_time - start_rf_time
return(rf_wflow)

}
saveRDS(fxn_random_forest,"fxn_random_forest.rds")
```

```{r trees=100, mtry(40,120),min_n(5,15),levels=23, set.seed(1234)}
fxn_random_forest_tuned = function(recipe, folds){
start_time <- Sys.time()

rft_model = rand_forest(mtry = tune(), min_n = tune(), trees = 100) %>% #add tuning of mtry and min_n parameters
   #setting trees to 100 here should also speed things up a bit, but more trees might be better
   set_engine("ranger", importance = "permutation") %>% #added importance metric
   set_mode("classification")
# 
rft_wflow = 
   workflow() %>% 
   add_model(rft_model) %>% 
   add_recipe(recipe)
# 
rft_grid = grid_regular(
   mtry(range = c(40, 120)), #these values determined through significant trial and error  # this came up as columns requested ie(predictors er msg:8 columns were requested but there were 7 predictors in the data. 7 will be used) 
   min_n(range = c(5, 15)), #these values determined through significant trial and error
   levels = 23
 )
# 
 set.seed(1234)
rft_res = tune_grid(    
   rft_wflow,
   resamples = folds,
   grid = rft_grid, #use the tuning grid
   control = ctrl_grid
 )
saveRDS(rft_res,"rft_res.rds")  #(about 11.18 mins)

end_time <- Sys.time()
round(end_time - start_time, digits = 2) # comment out if you want to print and not see the time difference of
#*run_time: `r round(endTime-startTime,digits =2)` seconds.*
return(rft_wflow)
}

saveRDS(fxn_random_forest_tuned,"fxn_random_forest_tuned.rds")
```



# Main()
```{r main}
# begin timer
start_time= Sys.time()

# random forest
workflow = random_forest(recipe, folds)
rf_res=readRDS("rf_res.rds")
accuracy_report(rf_res, workflow, train, test)
visuals(rf_res) #+labs(title = "Neural Network",subtitle = "recipe1", x = NULL, y = "Accuracy")
print("random forest complete")

# will need to look at the above visuals and tune before proceeding...
# random Forest Tuned
workflow_tuned = random_forest_tuned(recipe, folds)
rft_res=readRDS("rft_res.rds")
accuracy_report(rft_res, workflow_tuned, train, test)
visuals(rft_res)#+labs(title = "Neural Network",subtitle = "recipe1", x = NULL, y = "Accuracy")
print("random forest tuned complete")

# end timer and calculate
end_time= Sys.time()
end_time-start_time
```

