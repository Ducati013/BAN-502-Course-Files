---
title: "Function_Decision_trees"
author: "Kevin Foley"
date: "`r Sys.Date()`"
output: word_document
---

# Function
```{r  all the intro stuff}
library(tidyverse)
library(tidymodels)
library(caret)
library(mice)
library(VIM)
library(ranger)
library(RColorBrewer)
library(rpart)
library(rattle)
library(e1071)
library(xgboost)
library(usemodels)
library(nnet) #our neural network package
library(stacks)
library(vip) #variable importance

train = read.csv("C:/Users/ducat/Documents/UNCW_Grad_school/MIS 502 Predictive Analytics/Final Project2/Stacked Final individual pieces/ames_student-1.csv")

train = train %>% mutate_if(is.character,as_factor) 
set.seed(123) 
df_split = initial_split(train, prop = 0.7, strata = Above_Median) #70% in training
train = training(df_split)
test = testing(df_split)
set.seed(123)
folds = vfold_cv(train, v = 5)
recipe = recipe(Above_Median ~ ., train) %>%  
                step_normalize(all_predictors(),- all_nominal()) %>% # normalizes numeric predictors, not needed for categorical
                step_dummy(all_nominal(), -all_outcomes()) # to anything that is nominal(), but not the response
                           # all_numeric_predictors()) #<- another possible
                        # step_impute_mean(all_numeric_predictors()) %>%

recipe1 = recipe(Above_Median ~ Gr_Liv_Area + Year_Built + Full_Bath + First_Flr_SF + Total_Bsmt_SF + Garage_Area + Lot_Area + Garage_Type + Overall_Qual + Garage_Cars, train) %>%
   step_normalize(all_predictors(),- all_nominal()) %>%
     step_dummy(all_nominal(), -all_outcomes()) 
                       
recipe2 = recipe(Above_Median ~ Gr_Liv_Area + Year_Built + Full_Bath + First_Flr_SF + Total_Bsmt_SF + Garage_Area + Lot_Area + Garage_Type + Overall_Qual + Garage_Cars + Functional +Bsmt_Cond + Bsmt_Qual , train) %>%
   step_normalize(all_predictors(),- all_nominal()) %>%
     step_dummy(all_nominal(), -all_outcomes()) 
recipe3 = recipe(Above_Median ~ Gr_Liv_Area + Year_Built + Full_Bath + First_Flr_SF + Total_Bsmt_SF + Garage_Type + Overall_Qual, train) %>%
   step_normalize(all_predictors(),- all_nominal()) %>%
     step_dummy(all_nominal(), -all_outcomes()) 

ctrl_grid = control_stack_grid() #necessary for working with the stacks package
ctrl_res = control_stack_resamples() #necessary for working with the stacks package
```


```{r}
fxn_trees = function(recipe, folds){
 start_tm_time <- Sys.time()
 tree_model = decision_tree(cost_complexity = tune()) %>%
   set_engine("rpart", model = TRUE) %>%
   set_mode("classification")

 tree_recipe = recipe # %>%
#   step_normalize(all_predictors(), -all_nominal()) #+
#   #step_dummy(all_nominal(),-all_outcomes())
#
 tree_workflow = workflow() %>%
   add_model(tree_model) %>%
   add_recipe(tree_recipe)

 set.seed(1234)
 tree_res =
   tree_workflow %>%
   tune_grid(
     resamples = folds,
     grid = 25, #try 25 reasonable values for cp
     control = ctrl_grid #needed for stacking
     )
 end_tm_time <- Sys.time()
 end_tm_time - start_tm_time

 saveRDS(tree_res,"tree_res.rds")  # takes about(1 min)
 return(tree_workflow)
}
```

```{r}
tree_visuals = function(tree_res){
tree_res %>%
  collect_metrics() %>%
  ggplot(aes(cost_complexity, mean)) +
  geom_line(linewidth = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  labs(title="Descision Tree", subtitle="failure_recipe")
}
```

```{r}
accuracy_report = function(tree_res, workflow, train, test){

set.seed(123)  # this ensures we all get the same random forest
best_xgbt_res1 = select_best(tree_res, "accuracy")

final_xgbt_res1 = finalize_workflow(
  workflow,
  best_xgbt_res1
)

final_xgbt_res1
final_xgbt_res_fit1 = fit(final_xgbt_res1, train)
final_xgbt_res_fit1 %>% pull_workflow_fit() %>% vip(geom = "point")
trainpredrf = predict(final_xgbt_res_fit1, train)
auto_train =confusionMatrix(trainpredrf$.pred_class, train$Above_Median, 
                positive = "Yes")
t0 = data.matrix(auto_train)
auto_train_accuracy = (t0[1,1]+t0[2,2])/nrow(train)   # t0[row,col)]
auto_train_sensitivity = (t0[2,2]/(t0[2,2]+t0[1,2]))
testpredrf = predict(final_xgbt_res_fit1, test)
head(testpredrf)
auto_test = confusionMatrix(testpredrf$.pred_class, test$Above_Median, 
                positive = "Yes")
t00 = data.matrix(auto_test)
auto_test_accuracy = (t00[1,1]+t00[2,2])/nrow(test)
auto_test_sensitivity = (t00[2,2]/(t00[2,2]+t00[1,2]))
cat(strrep("*",15),"Train & Test: Accuracy & Sensitivity",strrep("*",15),'\n\n')
cat("Train accuracy:   ", round(auto_train_accuracy,digits=4),
    "\nTrain sensitivity:", round(auto_train_sensitivity,digits=4),
    #'\n\n',strrep("*",15),"Test parameters",strrep("*",15),
     "\n\nTest  accuracy:   ", round(auto_test_accuracy,digits=4),
    "\nTest sensitivity:", round(auto_test_sensitivity,digits=4),"\n\n")
# cat(strrep("*",15),"Train parameters",strrep("*",15),'\n')
# auto_train
# cat(strrep("*",15),"Test parameters",strrep("*",15),'\n')
# auto_test
cat(strrep("*",25),"END",strrep("*",25),'\n')
}
```

# Main()
```{r}
start_time = Sys.time()

saveRDS(accuracy_report,"accuracy_report.rds")
saveRDS(fxn_trees,"fxn_trees.rds")
saveRDS(tree_visuals,"tree_visuals.rds")

workflow = fxn_trees(recipe,folds)
tree_res = readRDS("tree_res.rds")
accuracy_report(tree_res, workflow, train,test)
tree_visuals(tree_res)




end_time = Sys.time()
end_time-start_time

```

