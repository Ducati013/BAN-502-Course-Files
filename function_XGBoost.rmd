---
title: "Function_ XGBoost"
author: "Kevin Foley"
date: "`r Sys.Date()`"
output: html_document
---
# Functions
```{r  all the intro stuff}
library(tidyverse)
library(tidymodels)
library(caret)
library(mice)
library(VIM)
library(ranger)
library(RColorBrewer)
library(rpart)
library(rattle)
library(e1071)
library(xgboost)
library(usemodels)
library(nnet) #our neural network package
library(stacks)
library(vip) #variable importance

train = read.csv("C:/Users/ducat/Documents/UNCW_Grad_school/MIS 502 Predictive Analytics/Final Project2/Stacked Final individual pieces/ames_student-1.csv")

train = train %>% mutate_if(is.character,as_factor) 
set.seed(123) 
df_split = initial_split(train, prop = 0.7, strata = Above_Median) #70% in training
train = training(df_split)
test = testing(df_split)
set.seed(123)
folds = vfold_cv(train, v = 5)
recipe = recipe(Above_Median ~ ., train) %>%  
                step_normalize(all_predictors(),- all_nominal()) %>% # normalizes numeric predictors, not needed for categorical
                step_dummy(all_nominal(), -all_outcomes()) # to anything that is nominal(), but not the response
                           # all_numeric_predictors()) #<- another possible
                        # step_impute_mean(all_numeric_predictors()) %>%

recipe1 = recipe(Above_Median ~ Gr_Liv_Area + Year_Built + Full_Bath + First_Flr_SF + Total_Bsmt_SF + Garage_Area + Lot_Area + Garage_Type + Overall_Qual + Garage_Cars, train) %>%
   step_normalize(all_predictors(),- all_nominal()) %>%
     step_dummy(all_nominal(), -all_outcomes()) 
                       
ctrl_grid = control_stack_grid() #necessary for working with the stacks package
ctrl_res = control_stack_resamples() #necessary for working with the stacks package
```

```{r accuracy report}
accuracy_report = function(nn_res, workflow, train, test){

set.seed(123)  # this ensures we all get the same random forest
best_xgbt_res1 = select_best(nn_res, "accuracy")

final_xgbt_res1 = finalize_workflow(
  workflow,
  best_xgbt_res1
)

final_xgbt_res1
final_xgbt_res_fit1 = fit(final_xgbt_res1, train)
final_xgbt_res_fit1 %>% pull_workflow_fit() %>% vip(geom = "point")
trainpredrf = predict(final_xgbt_res_fit1, train)
auto_train =confusionMatrix(trainpredrf$.pred_class, train$Above_Median, 
                positive = "Yes")
t0 = data.matrix(auto_train)
auto_train_accuracy = (t0[1,1]+t0[2,2])/nrow(train)   # t0[row,col)]
auto_train_sensitivity = (t0[2,2]/(t0[2,2]+t0[1,2]))
testpredrf = predict(final_xgbt_res_fit1, test)
head(testpredrf)
auto_test = confusionMatrix(testpredrf$.pred_class, test$Above_Median, 
                positive = "Yes")
t00 = data.matrix(auto_test)
auto_test_accuracy = (t00[1,1]+t00[2,2])/nrow(test)
auto_test_sensitivity = (t00[2,2]/(t00[2,2]+t00[1,2]))
cat(strrep("*",15),"Train & Test: Accuracy & Sensitivity",strrep("*",15),'\n\n')
cat("Train accuracy:   ", round(auto_train_accuracy,digits=4),
    "\nTrain sensitivity:", round(auto_train_sensitivity,digits=4),
    #'\n\n',strrep("*",15),"Test parameters",strrep("*",15),
     "\n\nTest  accuracy:   ", round(auto_test_accuracy,digits=4),
    "\nTest sensitivity:", round(auto_test_sensitivity,digits=4),"\n\n")
# cat(strrep("*",15),"Train parameters",strrep("*",15),'\n')
# auto_train
# cat(strrep("*",15),"Test parameters",strrep("*",15),'\n')
# auto_test
cat(strrep("*",25),"END",strrep("*",25),'\n')
}
```


```{r  xgboost function}
fxn_xgboost = function(recipe, folds){
start_time = Sys.time() #for timing
#
xgboost_recipe <- recipe %>%
   step_novel(all_nominal(), -all_outcomes()) %>%
   step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
   step_zv(all_predictors())
#
xgboost_spec <-
   boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(),
     loss_reduction = tune(), sample_size = tune()) %>%
   set_mode("classification") %>%
   set_engine("xgboost")
#
xgboost_workflow <-
   workflow() %>%
   add_recipe(xgboost_recipe) %>%
   add_model(xgboost_spec)
#
set.seed(1234)
 xgb_res <-
   tune_grid(xgboost_workflow,
             resamples = folds,
             grid = 25,
             control = ctrl_grid)
#
end_time = Sys.time()
end_time-start_time


saveRDS(xgb_res,"xgb_res.rds") # so you don't have to run the model again,because it may take some time( about 21 mins)
return(xgboost_workflow)
}
saveRDS(fxn_xgboost,"fxn_xgboost.rds")
```

```{r  xgboost tuned function}
fxn_xgboost_tuned = function(recipe, folds){
  start_time = Sys.time() #for timing
# # 
tgrid = expand.grid(
    trees = 100, #50, 100, and 150 in default 
    min_n = 1, #fixed at 1 as default 
    tree_depth = c(1,2,3,4), #1, 2, and 3 in default 
    learn_rate = c(0.01, 0.1, 0.2, 0.3, 0.4), #0.3 and 0.4 in default 
    loss_reduction = 0, #fixed at 0 in default 
    sample_size = c(0.5, 0.8, 1)) #0.5, 0.75, and 1 in default, 
# # 
xgboost_recipe <- recipe %>% 
    #recipe(formula = failure ~ ., data = train) %>% 
    step_novel(all_nominal(), -all_outcomes()) %>% 
    step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>% 
    step_zv(all_predictors()) 
# # 
xgboost_spec <- 
    boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), 
      loss_reduction = tune(), sample_size = tune()) %>% 
    set_mode("classification") %>% 
    set_engine("xgboost") 
# # 
xgboost_workflow <- 
    workflow() %>% 
    add_recipe(xgboost_recipe) %>% 
    add_model(xgboost_spec) 
# # 
set.seed(1234)
xgbt_res <-
    tune_grid(xgboost_workflow, 
              resamples = folds, 
              grid = tgrid,
              control = ctrl_grid)
# # 
end_time = Sys.time()
end_time-start_time
# 
# # Saving  # comment out once ran
saveRDS(xgbt_res,"xgbt_res.rds")  # about 3.82 mins
return(xgboost_workflow)
}
saveRDS(fxn_xgboost_tuned,"fxn_xgboost_tuned.rds")
```

# Main()
```{r}

# begin neural network model
workflow = fxn_xgboost(recipe,folds)
xgb_res = readRDS("xgb_res.rds")
accuracy_report(xgb_res, workflow, train, test)

# begin tuned neural network model
workflow_tuned = tuned_neural_network(recipe, folds)
xgbt_res = readRDS("xgbt_res.rds")
accuracy_report(xgbt_res, workflow_tuned, train,test)


```

